{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63558e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 9877 unique video IDs from D:\\SocialNetwork\\Code rieng\\P2_Youtube_VideosVI.csv\n",
      "Need to fetch 9877 remaining videos.\n",
      "Total batches: 198 (50 videos each)\n",
      "üíæ Saved partial (1000/9877 videos)\n",
      "üíæ Saved partial (2000/9877 videos)\n",
      "üíæ Saved partial (3000/9877 videos)\n",
      "üíæ Saved partial (4000/9877 videos)\n",
      "üíæ Saved partial (5000/9877 videos)\n",
      "üíæ Saved partial (6000/9877 videos)\n",
      "üíæ Saved partial (7000/9877 videos)\n",
      "üíæ Saved partial (8000/9877 videos)\n",
      "üíæ Saved partial (9000/9877 videos)\n",
      "üíæ Saved partial (9877/9877 videos)\n",
      "Rebuild complete.\n",
      "Final: 9877 videos written to D:\\SocialNetwork\\Code rieng\\P2_Youtube_VideosVI_rebuild.csv\n",
      "Missing/deleted videos: 170\n"
     ]
    }
   ],
   "source": [
    "from googleapiclient.discovery import build\n",
    "import pandas as pd, time, os, random\n",
    "\n",
    "# --- CONFIG ------------------------------------------------\n",
    "API_KEYS = [\n",
    "    \"AIzaSyD4_2mYEDO1PeBEptZyT2uW0FUETZmMAtA\",\n",
    "    \"AIzaSyBWk7hDdT5HFJBSA9QFkKlsNepq23aVIwk\",\n",
    "    \"AIzaSyBp7Rp4-X-7k0Lmr2ONgQvjMOeKs4rCQlQ\",\n",
    "    \"AIzaSyAPLj_aOuAdJmd5Kd8aGNMtjCbPmrK32os\",\n",
    "    \"AIzaSyDWL1eMBH1YA3-KyQSaujkpUDBr3Lv-hTU\",\n",
    "    \"AIzaSyDiY66E4RIkefCXAUM127DR2GbZgk7MYAs\",\n",
    "    \"AIzaSyAzuFvWoqktWZnzBjIbD0_zY90N4EaFNLU\",\n",
    "    \"AIzaSyAluVOntnG5IEB28hjXZ8IC4pCCq_fEY2Q\",\n",
    "    \"AIzaSyCzKyY7zuXfG2O1l7A8Qp55BajLebaY7rk\",\n",
    "    \"AIzaSyC_4OzFeeojxBwstQGmtNpZdzVESb-qI0s\",\n",
    "    \"AIzaSyCkOFKwB_xdyfR2rAVgYuhpKmIdztLjNu4\",\n",
    "    \"AIzaSyBxb7Dbwk-smT2Juq2M2eL7elp5dg3fSOI\",\n",
    "    \"AIzaSyBRJ1D9OHk91z8z8PTNSDHEW3YUGb13MwM\",\n",
    "    \"AIzaSyB8HWzS1BSTWPtRmWr4EnFdQPHlrUXxaeI\",\n",
    "    \"AIzaSyD-LALw0dfp_JjnUhdlSZ1qFtdHcXAGSL4\",\n",
    "    \"AIzaSyDHSl_tfIic8diP02yZ1WEZ5uKCi55ANv8\",\n",
    "    \"AIzaSyBqwdNo5oljo0cxPDpbKFTiJ51ixkE0FSY\",\n",
    "    \"AIzaSyCjS9PzkDmvpFH0ykqDeglPSyq4KMty1kI\",\n",
    "    \"AIzaSyCKWDKHLX_enEb1eFRbC0-xVcl0zUhdc2w\",\n",
    "    \"AIzaSyCfsG6_iDcsQIfb9ZJeswhktYjJZnPEffM\",\n",
    "    \"AIzaSyDAiIQIeNud14wQy4Zb7b77yLChF9pD8Mk\",\n",
    "    \"AIzaSyBdf0W3FYGK5KD11zLYqTG03v1-a96OQIM\",\n",
    "    \"AIzaSyBoDOqkgkCIZTsK2914FSS6DLhxNcN04Oo\",\n",
    "    \"AIzaSyDTS-ai8FAEmyh-pdr2AaWFzAe1J6w5P5c\",\n",
    "    \"AIzaSyCqVz4KkZW3_2BeR94QsQLoY6bXHFyPJsY\",\n",
    "    \"AIzaSyAQozqoFvPYwPVbygGkyeOIBUvEc_oP-uA\",\n",
    "    \"AIzaSyARIo46RgATDYupvQaTmNtEHXy6MWtN0YM\",\n",
    "    \"AIzaSyBcv2mXoYW1jKkFhD8n_A8UHcVVg1SOl-0\",\n",
    "    \"AIzaSyB11g0Zz4KClnzpo-94cWbcYJH1D92Rv1g\",\n",
    "    \"AIzaSyB2QqzzeBKo8q7FnRvS7paZvk4iawd0Uiw\"\n",
    "]\n",
    "INPUT_CSV = r\"D:\\SocialNetwork\\Code rieng\\P2_Youtube_VideosVI.csv\"\n",
    "OUTPUT_CSV = r\"D:\\SocialNetwork\\Code rieng\\P2_Youtube_VideosVI_rebuild.csv\"\n",
    "SAVE_EVERY_BATCHES = 20   # l∆∞u t·∫°m sau m·ªói 20 batch (50*20 = 1000 videos)\n",
    "BATCH_SIZE = 50\n",
    "\n",
    "# --- SETUP -------------------------------------------------\n",
    "key_index = 0\n",
    "youtube = build(\"youtube\", \"v3\", developerKey=API_KEYS[key_index])\n",
    "\n",
    "def switch_key():\n",
    "    \"\"\"Chuy·ªÉn sang API key kh√°c n·∫øu quota h·∫øt\"\"\"\n",
    "    global key_index, youtube\n",
    "    key_index = (key_index + 1) % len(API_KEYS)\n",
    "    youtube = build(\"youtube\", \"v3\", developerKey=API_KEYS[key_index])\n",
    "    print(f\"Switched to API key #{key_index+1}\")\n",
    "\n",
    "def safe_execute(func, *args, **kwargs):\n",
    "    \"\"\"Th·ª±c hi·ªán request an to√†n v√† xoay key n·∫øu quota l·ªói\"\"\"\n",
    "    for _ in range(len(API_KEYS)):\n",
    "        try:\n",
    "            return func(*args, **kwargs)\n",
    "        except Exception as e:\n",
    "            msg = str(e).lower()\n",
    "            if \"quota\" in msg or \"403\" in msg or \"rate\" in msg:\n",
    "                print(\"Quota error, switching key...\")\n",
    "                switch_key()\n",
    "                time.sleep(2)\n",
    "                continue\n",
    "            else:\n",
    "                print(\"Error:\", e)\n",
    "                return None\n",
    "    return None\n",
    "\n",
    "# --- LOAD VIDEO IDs ----------------------------------------\n",
    "df_in = pd.read_csv(INPUT_CSV, dtype=str)\n",
    "if \"video_id\" not in df_in.columns:\n",
    "    raise SystemExit(\"File kh√¥ng c√≥ c·ªôt 'video_id'!\")\n",
    "\n",
    "video_ids = df_in[\"video_id\"].dropna().unique().tolist()\n",
    "print(f\"Loaded {len(video_ids)} unique video IDs from {INPUT_CSV}\")\n",
    "\n",
    "# --- IF resume ---------------------------------------------\n",
    "if os.path.exists(OUTPUT_CSV):\n",
    "    df_prev = pd.read_csv(OUTPUT_CSV, dtype=str)\n",
    "    done_ids = set(df_prev[\"video_id\"])\n",
    "    print(f\"Found {len(done_ids)} previously fetched, resuming...\")\n",
    "else:\n",
    "    done_ids = set()\n",
    "\n",
    "to_fetch = [vid for vid in video_ids if vid not in done_ids]\n",
    "print(f\"Need to fetch {len(to_fetch)} remaining videos.\")\n",
    "\n",
    "# --- MAIN LOOP ---------------------------------------------\n",
    "results = []\n",
    "batches = [to_fetch[i:i+BATCH_SIZE] for i in range(0, len(to_fetch), BATCH_SIZE)]\n",
    "print(f\"Total batches: {len(batches)} (50 videos each)\")\n",
    "\n",
    "for i, batch in enumerate(batches, 1):\n",
    "    req = lambda: youtube.videos().list(\n",
    "        part=\"snippet,statistics,contentDetails\",\n",
    "        id=\",\".join(batch),\n",
    "        maxResults=BATCH_SIZE\n",
    "    )\n",
    "    res = safe_execute(lambda: req().execute())\n",
    "    if not res:\n",
    "        print(f\"Batch {i} failed, skipping.\")\n",
    "        continue\n",
    "\n",
    "    items = res.get(\"items\", [])\n",
    "    fetched_ids = set([it[\"id\"] for it in items])\n",
    "    missing = [vid for vid in batch if vid not in fetched_ids]\n",
    "\n",
    "    for it in items:\n",
    "        stats = it.get(\"statistics\", {})\n",
    "        snip = it.get(\"snippet\", {})\n",
    "        cont = it.get(\"contentDetails\", {})\n",
    "        results.append({\n",
    "            \"video_id\": it[\"id\"],\n",
    "            \"title\": snip.get(\"title\"),\n",
    "            \"description\": snip.get(\"description\"),\n",
    "            \"published_at\": snip.get(\"publishedAt\"),\n",
    "            \"channel\": snip.get(\"channelTitle\"),\n",
    "            \"category_id\": snip.get(\"categoryId\"),\n",
    "            \"tags\": \",\".join(snip.get(\"tags\", [])) if \"tags\" in snip else None,\n",
    "            \"view_count\": stats.get(\"viewCount\"),\n",
    "            \"like_count\": stats.get(\"likeCount\"),\n",
    "            \"comment_count\": stats.get(\"commentCount\")\n",
    "        })\n",
    "\n",
    "    for vid in missing:\n",
    "        results.append({\n",
    "            \"video_id\": vid,\n",
    "            \"title\": None,\n",
    "            \"description\": None,\n",
    "            \"published_at\": None,\n",
    "            \"channel\": None,\n",
    "            \"category_id\": None,\n",
    "            \"tags\": None,\n",
    "            \"view_count\": None,\n",
    "            \"like_count\": None,\n",
    "            \"comment_count\": None\n",
    "        })\n",
    "\n",
    "    # l∆∞u t·∫°m\n",
    "    if i % SAVE_EVERY_BATCHES == 0 or i == len(batches):\n",
    "        df_new = pd.DataFrame(results)\n",
    "        if os.path.exists(OUTPUT_CSV):\n",
    "            df_old = pd.read_csv(OUTPUT_CSV, dtype=str)\n",
    "            df_all = pd.concat([df_old, df_new], ignore_index=True).drop_duplicates(subset=\"video_id\")\n",
    "        else:\n",
    "            df_all = df_new\n",
    "        df_all.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "        print(f\"üíæ Saved partial ({len(df_all)}/{len(video_ids)} videos)\")\n",
    "        results = []\n",
    "\n",
    "    time.sleep(random.uniform(0.7, 1.2))\n",
    "\n",
    "print(\"Rebuild complete.\")\n",
    "df_final = pd.read_csv(OUTPUT_CSV, dtype=str)\n",
    "print(f\"Final: {len(df_final)} videos written to {OUTPUT_CSV}\")\n",
    "missing_count = df_final[\"title\"].isna().sum()\n",
    "print(f\"Missing/deleted videos: {missing_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "24a379c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current: 9834 videos. Need to crawl 166 more.\n",
      "Start crawling more videos...\n",
      "Searching keyword: M√°y t√≠nh\n",
      "Fetched 6/166 new videos.\n",
      "Searching keyword: ƒêi·ªán tho·∫°i\n",
      "Fetched 27/166 new videos.\n",
      "Searching keyword: ·∫®m th·ª±c\n",
      "Fetched 49/166 new videos.\n",
      "Searching keyword: C√¥ng ngh·ªá\n",
      "Fetched 65/166 new videos.\n",
      "Searching keyword: Game\n",
      "Fetched 93/166 new videos.\n",
      "Searching keyword: Stream\n",
      "Fetched 134/166 new videos.\n",
      "Searching keyword: ·∫®m th·ª±c\n",
      "Fetched 135/166 new videos.\n",
      "Searching keyword: ·∫®m th·ª±c\n",
      "Fetched 136/166 new videos.\n",
      "Searching keyword: ƒê√°nh gi√°\n",
      "Fetched 157/166 new videos.\n",
      "Searching keyword: ·∫®m th·ª±c\n",
      "Fetched 157/166 new videos.\n",
      "Searching keyword: B√≥ng ƒë√°\n",
      "Fetched 197/166 new videos.\n",
      "Crawl complete. Total 10031 videos saved.\n"
     ]
    }
   ],
   "source": [
    "from googleapiclient.discovery import build\n",
    "import pandas as pd, random, time, os\n",
    "\n",
    "OUTPUT_CSV = r\"D:\\SocialNetwork\\Code rieng\\P2_Youtube_VideosVI_rebuild.csv\"\n",
    "TARGET_TOTAL = 10000\n",
    "\n",
    "# --- LOAD CURRENT DATA -------------------------------------\n",
    "df = pd.read_csv(OUTPUT_CSV, dtype=str)\n",
    "current = len(df)\n",
    "need = TARGET_TOTAL - current\n",
    "print(f\"Current: {current} videos. Need to crawl {need} more.\")\n",
    "\n",
    "if need <= 0:\n",
    "    print(\"Already reached 10,000 videos. Skip crawling.\")\n",
    "else:\n",
    "    # --- CONFIG --------------------------------------------\n",
    "    API_KEYS = [\n",
    "        \"AIzaSyDTS-ai8FAEmyh-pdr2AaWFzAe1J6w5P5c\",\n",
    "        \"AIzaSyCqVz4KkZW3_2BeR94QsQLoY6bXHFyPJsY\",\n",
    "        \"AIzaSyAQozqoFvPYwPVbygGkyeOIBUvEc_oP-uA\",\n",
    "    ]\n",
    "    key_idx = 0\n",
    "    youtube = build(\"youtube\", \"v3\", developerKey=API_KEYS[key_idx])\n",
    "\n",
    "    def switch_key():\n",
    "        \"\"\"Chuy·ªÉn sang API key kh√°c n·∫øu quota h·∫øt\"\"\"\n",
    "        global key_idx, youtube\n",
    "        key_idx = (key_idx + 1) % len(API_KEYS)\n",
    "        youtube = build(\"youtube\", \"v3\", developerKey=API_KEYS[key_idx])\n",
    "        print(f\"Switched to API key #{key_idx+1}\")\n",
    "\n",
    "    def safe_search(**kwargs):\n",
    "        \"\"\"G·ªçi API search an to√†n, t·ª± xoay key n·∫øu quota l·ªói\"\"\"\n",
    "        for _ in range(len(API_KEYS)):\n",
    "            try:\n",
    "                return youtube.search().list(**kwargs).execute()\n",
    "            except Exception as e:\n",
    "                if \"quota\" in str(e).lower() or \"403\" in str(e).lower():\n",
    "                    print(\"Quota error, switching key...\")\n",
    "                    switch_key()\n",
    "                    time.sleep(2)\n",
    "                    continue\n",
    "                else:\n",
    "                    print(\"Error:\", e)\n",
    "                    return None\n",
    "        return None\n",
    "\n",
    "    # --- KEYWORDS ------------------------------------------\n",
    "    KEYWORDS = [\n",
    "        \"Du l·ªãch\", \"C√¥ng ngh·ªá\", \"B√≥ng ƒë√°\", \"·∫®m th·ª±c\", \"Stream\",\n",
    "        \"Th·ªß thu·∫≠t\", \"Gi·∫£i tr√≠\", \"Tin t·ª©c\", \"Phim\", \"Ho·∫°t h√¨nh\", \"Y t·∫ø\", \"H·ªçc t·∫≠p\", \"ƒêi·ªán tho·∫°i\",\n",
    "        \"√Çm nh·∫°c\", \"Review\", \"H∆∞·ªõng d·∫´n\", \"ƒê√°nh gi√°\", \"M√°y t√≠nh\", \"Vlog\",\n",
    "        \"Game\", \"L√†m ƒë·∫πp\"\n",
    "    ]\n",
    "\n",
    "    crawled = []\n",
    "    existing_ids = set(df[\"video_id\"])\n",
    "    attempts = 0\n",
    "\n",
    "    print(\"Start crawling more videos...\")\n",
    "    while len(crawled) < need and attempts < 80:\n",
    "        random.shuffle(KEYWORDS)\n",
    "        kw = random.choice(KEYWORDS)\n",
    "        print(f\"Searching keyword: {kw}\")\n",
    "\n",
    "        res = safe_search(\n",
    "            q=kw, part=\"id,snippet\", regionCode=\"VN\",\n",
    "            relevanceLanguage=\"vi\", maxResults=50, type=\"video\"\n",
    "        )\n",
    "        if not res:\n",
    "            attempts += 1\n",
    "            continue\n",
    "\n",
    "        for item in res.get(\"items\", []):\n",
    "            vid = item[\"id\"][\"videoId\"]\n",
    "            if vid not in existing_ids:\n",
    "                existing_ids.add(vid)\n",
    "                crawled.append({\n",
    "                    \"video_id\": vid,\n",
    "                    \"title\": item[\"snippet\"].get(\"title\"),\n",
    "                    \"description\": item[\"snippet\"].get(\"description\"),\n",
    "                    \"published_at\": item[\"snippet\"].get(\"publishedAt\"),\n",
    "                    \"channel\": item[\"snippet\"].get(\"channelTitle\"),\n",
    "                    \"category_id\": None,\n",
    "                    \"tags\": None,\n",
    "                    \"view_count\": None,\n",
    "                    \"like_count\": None,\n",
    "                    \"comment_count\": None\n",
    "                })\n",
    "\n",
    "        print(f\"Fetched {len(crawled)}/{need} new videos.\")\n",
    "        attempts += 1\n",
    "        time.sleep(random.uniform(1.0, 2.0))\n",
    "\n",
    "    # --- SAVE MERGED FILE -----------------------------------\n",
    "    if crawled:\n",
    "        df_new = pd.DataFrame(crawled)\n",
    "        df_final = pd.concat([df, df_new], ignore_index=True)\n",
    "        df_final.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "        print(f\"Crawl complete. Total {len(df_final)} videos saved.\")\n",
    "    else:\n",
    "        print(\"No new videos were crawled.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "74e64ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10031 videos.\n",
      "Need to refetch 3393 videos missing stats/tags.\n",
      "Updated metadata for 3390 videos.\n",
      "Saved back to D:\\SocialNetwork\\Code rieng\\P2_Youtube_VideosVI_rebuild.csv\n"
     ]
    }
   ],
   "source": [
    "from googleapiclient.discovery import build\n",
    "import pandas as pd, time, random\n",
    "\n",
    "OUTPUT_CSV = r\"D:\\SocialNetwork\\Code rieng\\P2_Youtube_VideosVI_rebuild.csv\"\n",
    "\n",
    "API_KEYS = [\n",
    "    \"AIzaSyDTS-ai8FAEmyh-pdr2AaWFzAe1J6w5P5c\",\n",
    "    \"AIzaSyCqVz4KkZW3_2BeR94QsQLoY6bXHFyPJsY\",\n",
    "    \"AIzaSyAQozqoFvPYwPVbygGkyeOIBUvEc_oP-uA\",\n",
    "]\n",
    "\n",
    "# --- Setup ---\n",
    "key_idx = 0\n",
    "youtube = build(\"youtube\", \"v3\", developerKey=API_KEYS[key_idx])\n",
    "\n",
    "def switch_key():\n",
    "    global key_idx, youtube\n",
    "    key_idx = (key_idx + 1) % len(API_KEYS)\n",
    "    youtube = build(\"youtube\", \"v3\", developerKey=API_KEYS[key_idx])\n",
    "    print(f\"Switched to API key #{key_idx+1}\")\n",
    "\n",
    "def safe_execute(func, *args, **kwargs):\n",
    "    for _ in range(len(API_KEYS)):\n",
    "        try:\n",
    "            return func(*args, **kwargs)\n",
    "        except Exception as e:\n",
    "            if \"quota\" in str(e).lower() or \"403\" in str(e).lower():\n",
    "                print(\"Quota error, switching key...\")\n",
    "                switch_key()\n",
    "                time.sleep(2)\n",
    "                continue\n",
    "            else:\n",
    "                print(\"Error:\", e)\n",
    "                return None\n",
    "    return None\n",
    "\n",
    "# --- Load CSV ---\n",
    "df = pd.read_csv(OUTPUT_CSV, dtype=str)\n",
    "print(f\"Loaded {len(df)} videos.\")\n",
    "\n",
    "# L·ªçc video ch∆∞a c√≥ view_count ho·∫∑c tags\n",
    "mask = df[\"view_count\"].isna() | df[\"tags\"].isna()\n",
    "to_update = df.loc[mask, \"video_id\"].dropna().unique().tolist()\n",
    "print(f\"Need to refetch {len(to_update)} videos missing stats/tags.\")\n",
    "\n",
    "# --- Fetch in batches ---\n",
    "BATCH_SIZE = 50\n",
    "updated = []\n",
    "for i in range(0, len(to_update), BATCH_SIZE):\n",
    "    batch = to_update[i:i+BATCH_SIZE]\n",
    "    res = safe_execute(lambda: youtube.videos().list(\n",
    "        part=\"snippet,statistics,contentDetails\",\n",
    "        id=\",\".join(batch),\n",
    "        maxResults=BATCH_SIZE\n",
    "    ).execute())\n",
    "    if not res:\n",
    "        continue\n",
    "\n",
    "    for it in res.get(\"items\", []):\n",
    "        stats = it.get(\"statistics\", {})\n",
    "        snip = it.get(\"snippet\", {})\n",
    "        updated.append({\n",
    "            \"video_id\": it[\"id\"],\n",
    "            \"tags\": \"|\".join(snip.get(\"tags\", [])) if \"tags\" in snip else None,\n",
    "            \"category_id\": snip.get(\"categoryId\"),\n",
    "            \"view_count\": stats.get(\"viewCount\"),\n",
    "            \"like_count\": stats.get(\"likeCount\"),\n",
    "            \"comment_count\": stats.get(\"commentCount\")\n",
    "        })\n",
    "    time.sleep(random.uniform(0.8, 1.5))\n",
    "\n",
    "# --- Update DataFrame ---\n",
    "df_updates = pd.DataFrame(updated)\n",
    "df = df.merge(df_updates, on=\"video_id\", how=\"left\", suffixes=(\"\", \"_new\"))\n",
    "\n",
    "for col in [\"tags\", \"category_id\", \"view_count\", \"like_count\", \"comment_count\"]:\n",
    "    df[col] = df[col].combine_first(df[f\"{col}_new\"])\n",
    "    df.drop(columns=[f\"{col}_new\"], inplace=True, errors=\"ignore\")\n",
    "\n",
    "# --- Save back ---\n",
    "df.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"Updated metadata for {len(df_updates)} videos.\")\n",
    "print(f\"Saved back to {OUTPUT_CSV}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (newenv)",
   "language": "python",
   "name": "newenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
